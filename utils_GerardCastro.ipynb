{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253ae0fb",
   "metadata": {},
   "source": [
    "# 1. Custom classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5fa8e",
   "metadata": {},
   "source": [
    "Let us first describe my implementation of 2 (of the 3) main classes which drive the execution of the program. To fully understand them it is important to note that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d004d28",
   "metadata": {},
   "source": [
    "- They were amended to be parsed as [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)'s transformations: in particular, they (need to) **implement** the methods `fit` and `transform` such that:\n",
    "  - `fit` & `transform` have same signature: they accept the features' dataframe (and labels' to follow `sklearn` conventions, but it is not used)\n",
    "  - `fit` returns the class itself, while `transform` returns the transformation output (it can be a `pandas.DataFrame`, `numpy.ndarray` or `scipy.sparse.csr.csr_matrix`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3907b9",
   "metadata": {},
   "source": [
    "- The pippeline contains the following transformations prior to the model fitting:\n",
    "  1. Questions' text preprocessing, through `TextPreprocessor`class\n",
    "  2. Feature (and extra features) generation:\n",
    "    - 2.1. The feature vectors are found, for each question, with the `FeatureGenerator` class; and then they are aggregated to convert them in a single feature vector (subtracted, compared, stacked...).\n",
    "    - 2.2. The extra features are added in form of column to the former feature matrix with the `ExtraFeaturesCreator` (class initialized and called from `FeatureGenerator`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2c5c6",
   "metadata": {},
   "source": [
    "- Several methods are available for each part of the pipeline and they are listed/stored in private global variables (at module level). These are:\n",
    "  - Feature vector extraction: stored at `_SUPPORTED_EXTRACTORS`, these are:\n",
    "    - `CountVectorizer` (labeled as 'cv' and 'cv_2w' depending on `ngram_range`)\n",
    "    - `TF-IDF` (labeled as 'tf_idf' and 'tf_idf_2w' depending on `ngram_range`)\n",
    "    - `spacy` embedding (labeled as 'spacy_small' and 'spacy_medium', but they are DISMISSED due to computational loads)\n",
    "  - Feature vector aggregation: stored at `_SUPPORTED_AGGREGATORS`, these are:\n",
    "    - `stack`: horizontally stacking the feature vectors\n",
    "    - `absolute`: computing the absolute difference between feature vectors\n",
    "    - `cosine`: computing the cosine similarity between feature vectors (DISMISSED because the outputs were dense, not sparse, are did not fit in memory no matter what we tried)\n",
    "  - Extra features creation: stored at `_SUPPORTED_EXTRA_FEATURES`:\n",
    "    - Several functions available: every of them receives 2 list of words (`str`) representing the questions' words list (after preprocessing) and must return a `float`. Some examples, reviewed below, are: `_length_ratio`, `_get_coincident_words_ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c81e05",
   "metadata": {},
   "source": [
    "## 1.1. `FeatureGenerator` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d188187",
   "metadata": {},
   "source": [
    "```python\n",
    "class FeatureGenerator:\n",
    "    def __init__(self,\n",
    "                 exts: Tuple = ('cv', ),\n",
    "                 aggs: Tuple = ('stack', ),\n",
    "                 extra_features: Tuple[str] = 'all') -> None:\n",
    "        assert len(exts) == len(aggs), \\\n",
    "            \"Extractor and aggregator lists must be of the same length\"\n",
    "        self.extractors = [_get_extractor(ext) for ext in exts]\n",
    "        self.extractor_names: Tuple[str] = exts\n",
    "        self.aggregators = [_SUPPORTED_AGGREGATORS[agg] for agg in aggs]\n",
    "        self.extra_features_creator = ExtraFeaturesCreator(extra_features)\n",
    "\n",
    "    def set_params(self,\n",
    "                   exts: Tuple = ('cv', ),\n",
    "                   aggs: Tuple = ('stack', ),\n",
    "                   extra_features: Tuple[str] = 'all') -> None:\n",
    "        self.__class__(exts, aggs, extra_features)\n",
    "\n",
    "    def fit(self, questions_df: pd.DataFrame, y=None):\n",
    "        self.extractors = [ext if name.startswith('spacy') else ext.fit(\n",
    "            questions_df.values.flatten()) for name, ext in zip(\n",
    "            self.extractor_names, self.extractors)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, questions_df: pd.DataFrame, y=None):\n",
    "        agg_features = []\n",
    "        for name, ext, agg in zip(self.extractor_names, self.extractors,\n",
    "                                  self.aggregators):\n",
    "            # we apply the extractor to each question\n",
    "            if name.startswith('spacy'):\n",
    "                print(\"Using spacy word embedding: please WAIT, \"\n",
    "                      \"this may take some time\")\n",
    "                # then we use a spacy embedding\n",
    "                x_q1 = questions_df.iloc[:, 0].apply(\n",
    "                    lambda x: ext(x).vector)\n",
    "                x_q2 = questions_df.iloc[:, 1].apply(\n",
    "                    lambda x: ext(x).vector)\n",
    "\n",
    "            else:\n",
    "                x_q1 = ext.transform(questions_df.iloc[:, 0])\n",
    "                x_q2 = ext.transform(questions_df.iloc[:, 1])\n",
    "\n",
    "            # and we aggregate them\n",
    "            x_agg = agg(x_q1, x_q2)\n",
    "            agg_features.append(x_agg)\n",
    "\n",
    "        if len(self.extra_features_creator.features_functions) != 0:\n",
    "            # in parallel, we compute the extra features\n",
    "            x_extra: np.ndarray = self.extra_features_creator.transform(\n",
    "                questions_df)\n",
    "            # finally, we merge them\n",
    "            return hstack((hstack(agg_features), x_extra))\n",
    "        return hstack(agg_features)\n",
    "\n",
    "\n",
    "def _get_extractor(ext: str):\n",
    "    if ext in ['spacy_small', 'spacy_medium']:\n",
    "        _spacy_version: str = _SUPPORTED_EXTRACTORS[ext]\n",
    "        try:\n",
    "            import spacy\n",
    "            spacy.load(_spacy_version)\n",
    "        except OSError:\n",
    "            os.system(f'python -m spacy download {_spacy_version}')\n",
    "        finally:\n",
    "            import spacy\n",
    "            return spacy.load(_spacy_version)\n",
    "    else:\n",
    "        return _SUPPORTED_EXTRACTORS[ext]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a950065",
   "metadata": {},
   "source": [
    "In summary, the `transform` method of this class drives it usability and:\n",
    "- Extracts the feature vectors (given the dataframe of the preprocessed questions' text) for each sample\n",
    "- Aggregates them in a single feature vector (conforming a matrix, rows as samples)\n",
    "- Finally, adds the extra features (created with `ExtraFeaturesCreator`) to the aggregated feature vectors' matrix\n",
    "\n",
    "The `fit` method just fits the feature vector 'extractors' (i.e. TF-IDF and CV) with the whole text (of both questions, for all samples). Finally, the `__init__` method allows the user to select which techniques can be used for each of the former tasks.\n",
    "\n",
    "It is worth mentioning the utility that Claudia, [@claudia-hm](https://github.com/claudia-hm), implemented for this class:\n",
    "- **More than one kind of extraction and aggregation can be used**. Namely, we as feature vector we can set the stacking of 2 feature vectors: 1 coming from one kind of extraction/aggregation (i.e. 'cv' & 'stack') and 1 coming from another kind (i.e. 'tf_idf' & 'absolute'). This notably raises the achieved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5854b",
   "metadata": {},
   "source": [
    "## 1.2. `ExtraFeaturesCreator`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82695022",
   "metadata": {},
   "source": [
    "```python\n",
    "class ExtraFeaturesCreator:\n",
    "    def __init__(self, features_to_add: Union[Tuple, str]) -> None:\n",
    "        if isinstance(features_to_add, str):\n",
    "            assert features_to_add == 'all', \"Unrecognized extra features list\"\n",
    "            self.features_functions: Dict[str, callable] = \\\n",
    "                _SUPPORTED_EXTRA_FEATURES\n",
    "        else:\n",
    "            self.features_functions = {\n",
    "                _n: _SUPPORTED_EXTRA_FEATURES[_n] for _n in features_to_add}\n",
    "\n",
    "    def transform(self, questions_df: pd.DataFrame) -> np.ndarray:\n",
    "        if len(self.features_functions) == 0:\n",
    "            raise ValueError(\"There is no extra features to be aggregated\")\n",
    "\n",
    "        for _c in ('question1', 'question2'):\n",
    "            questions_df[_c] = questions_df[_c].str.split()\n",
    "        extra_features = pd.DataFrame()\n",
    "\n",
    "        for _f_name, _f_function in self.features_functions.items():\n",
    "            extra_features[_f_name] = questions_df.apply(\n",
    "                lambda x: _f_function(x.question1, x.question2), axis=1)\n",
    "\n",
    "        return extra_features.values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae7ccb",
   "metadata": {},
   "source": [
    "Since it is called from `FeatureGenerator`, and not from `sklearn.pipeline.Pipeline`, just needs to implement a `transform` method (no `fit`). This simply receives a dataframe containing the 2 questions' text and, for each selected technique to generate extra features, computes (as column) the extra feature vectors. These are finally returned as `np.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efad14",
   "metadata": {},
   "source": [
    "# 2. Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320888c3",
   "metadata": {},
   "source": [
    "Henceforward, the functions I implemented are presented. To be exact, their docstrings, in which my explanation is includedm are shown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebcafa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_nan_questions, _horizontal_stacking, _length_ratio, _get_coincident_words_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68109d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function remove_nan_questions in module utils:\n",
      "\n",
      "remove_nan_questions(x_train: pandas.core.frame.DataFrame, y_train: pandas.core.frame.DataFrame) -> Tuple[pandas.core.frame.DataFrame, pandas.core.frame.DataFrame]\n",
      "    Remove those samples which contain a NaN in at least one question.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x_train: pd.DataFrame\n",
      "        Two columns dataframe containing the feature questions\n",
      "    y_train: pd.DataFrame\n",
      "        One column dataframe containing the labels\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dropped_x_train, dropped_y_train: Tuple[pd.DataFrame, pd.DataFrame]\n",
      "        Dataframes without NaN in any sample\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(remove_nan_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706ae4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function _horizontal_stacking in module utils:\n",
      "\n",
      "_horizontal_stacking(x_q1: scipy.sparse.csr.csr_matrix, x_q2: scipy.sparse.csr.csr_matrix) -> scipy.sparse.csr.csr_matrix\n",
      "    Stack horizontally the 2 passed feature matrices\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x_q1: csr_matrix\n",
      "        Feature (sparse) matrix (each row is the feature vector\n",
      "        obtained from the first question)\n",
      "    x_q2: csr_matrix\n",
      "        Feature (sparse) matrix of the second question\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Feature (sparse) matrix with the questions merged\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(_horizontal_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81232778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function _length_ratio in module utils:\n",
      "\n",
      "_length_ratio(q1_w: List[str], q2_w: List[str]) -> float\n",
      "    Return the question's length ratio (the first with respect the second).\n",
      "    If any of them has 0 length (after preprocessing), the retrieved ratio\n",
      "    is 0.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    q1_w: List[str]\n",
      "        List of words contained in the first (preprocessed) question's samples\n",
      "    q2_w: List[str]\n",
      "        List of words contained in the second (preprocessed) question's samples\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ratio: float\n",
      "        Question's length ratio\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(_length_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c1bf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function _get_coincident_words_ratio in module utils:\n",
      "\n",
      "_get_coincident_words_ratio(q1_w: List[str], q2_w: List[str]) -> float\n",
      "    Count the ratio of coincident words with respect the total number of them.\n",
      "    This is applied at a sample level.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    q1_w: List[str]\n",
      "        List of words contained in the first (preprocessed) question's samples\n",
      "    q2_w: List[str]\n",
      "        List of words contained in the second (preprocessed) question's samples\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ratio: float\n",
      "        Ratio of coincident words (between the 2 questions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(_get_coincident_words_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
