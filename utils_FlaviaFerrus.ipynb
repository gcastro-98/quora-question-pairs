{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c732fd9-18f3-4229-8221-e287a9bf2089",
   "metadata": {},
   "source": [
    "# Project structure\n",
    "\n",
    "This project was based on generating a baseline solution and an improved solution for the Quora Challenge in order to apply the knowledge acquired throughut the Natural Language Processing course of the MsC in Data Science, UB. The structure of this project was based on constructing the Preprocessing functions, generating the corresponding Extra Features from the question pairs, and developing an optimal model by going through different classification models in order to minimize the prediction error.\n",
    "\n",
    "We aimed to build a solid model in order to create a baseline for solving Natural Language Processing (NLP) problems. In order to do so, we adopted an approach with custom classes and the definition of a pipeline in order to store and organise our different models. Once this basic skeleton was robust and well-structured, we started innovating and trying some new extra features, combining different text vectorizers along with different aggregation function, as well as implementing diverse classifiers. The results did beat our expectations, getting accuracy values around 0.84, and AUC of 0.092, for the test set. Knowing that there is a lot to improve and so many further studies we could conduct, we have developed and improved a basic structure for general problems involving NLP that we could for sure improve and find useful in future studies or classifying problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd888dd-b45b-4385-baf8-f17da869abb0",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "\n",
    "We all contributed on the elaboration of the general structure of the project and the code. I personally contributed on including different ideas and implementations for the text vectorizers (tf-idf and cv with multiple binings), as well as some pre processing functions (such as the remove punctuation function, some custom stop word removers). \n",
    "\n",
    "Additionally, I was also involved in optimizing the cosine similarity function, since it did not provide a efficient sparse matrix output in order to stack the obtained model along with the remaining features we had, already encoded in a sparse matrix form. Some further studies could be centerd on trying to optimize the cosine similarity funciton using `cython`. However, in this pilot model this could not be possible. \n",
    "\n",
    "Moreover, I mainly contributed on generating new extra features presented as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79c9a9-f2e0-4586-87b1-0e479bc31aaf",
   "metadata": {},
   "source": [
    "## Extra feature generation\n",
    "\n",
    "Some extra features we have also considered on the generation of the training seting order to get better results and so we take into account different information derived from the original data set. The most relevant extra features: \n",
    "- **Get coincident keywords**: feature that compares the intersection of the set of key words, this is, key question words, such as What, Where, Which… with each question among the two strings compared. This feature was designed taking into account that similar questions would start with the same question keyword. It returns 1 when they start or contain (they would mostly contain it at the beginning as intended) with the same word, and 0 if not. \n",
    "- **Jaccard distance**: feature that computes the Jaccard distance between two strings. It is based on computing the ratio between the length of the intersection set of words from each question string over the length of the union set of words between both questions.\n",
    "- **Levenshtein distance**: feature that computes the Levenshtein distance between the two given questions. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. In this case, we may consider the implementation of two variations\n",
    "    - By words: we adapt the Levenshtein algorithm to considering the number of words we would need to change in order to get the exact similar sentence in both input questions.  \n",
    "    - Regular Levenshtein distance but considering the whole question, and studying letter by letter each character. This feature has problems of memory capacity and it tends to outload our computers. Therefore, the more optimal application of the Levenshtein distance is the custom distance computed for considering the word comparison. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6936e-d15f-4689-af7e-43d58a14532b",
   "metadata": {},
   "source": [
    "Finally, I worked on the generation of the report in order to conduct a concise and structured basis with all the information and the followed approach. We all also contributed on trying different models and combinations of features, in order to complete the information driven from the grid, which was higly demanding computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702302fc",
   "metadata": {},
   "source": [
    "#### Further steps of improvement:\n",
    "\n",
    "Word Mover’s Distance (WMD) enables us to assess the “distance” between two documents in a meaningful way even when they have no words in common. It uses word2vec vector embeddings of words. \n",
    "\n",
    "WMD is based on recent results in word embeddings that learn semantically meaningful representations for words from local co-occurrences in sentences. WMD leverages the results of advanced embedding techniques like word2vec and Glove, which generates word embeddings of unprecedented quality and scales naturally to very large data sets. The distance between two text documents A and B is calculated by the minimum cumulative distance that words from the text document A needs to travel to match exactly the point cloud of text document B.\n",
    "\n",
    "This needs an external set of words to create the semantic metrics! \n",
    "\n",
    "Other possible studies could be considering a BK-Tree but considering words instead of letters in order to define the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb540a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
