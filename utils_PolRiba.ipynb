{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37e910df",
   "metadata": {},
   "source": [
    "# CONTRIBUTIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a70b5efb",
   "metadata": {},
   "source": [
    "I mainly worked on the theoretical part.\n",
    "\n",
    "First of all I tried to understand which distance would be the optimal one for our project and helped to implement them.\n",
    "\n",
    "I tried the following distances:\n",
    "\n",
    "Cosine similarity: This metric is commonly used in natural language processing (NLP) to compare the similarity between two text documents. It measures the cosine of the angle between two vectors, where the vectors represent the frequency of each word in the documents. The cosine similarity score ranges from -1 to 1, with a score of 1 indicating perfect similarity and a score of -1 indicating total dissimilarity.\n",
    "\n",
    "Jaccard similarity: This metric is used to compare the similarity between two sets of items. In the context of NLP, the sets can represent the words in two questions. The Jaccard similarity is defined as the size of the intersection divided by the size of the union of the sets. The score ranges from 0 to 1, with a score of 1 indicating perfect similarity.\n",
    "\n",
    "Levenshtein distance: This metric measures the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one question into another. This distance can be useful for comparing short questions that have a similar structure.\n",
    "\n",
    "Euclidean distance: This metric is commonly used in machine learning to measure the distance between two data points in a high-dimensional space. In the context of comparing questions, each question can be represented as a vector of features, such as the frequency of each word in the question. The Euclidean distance between the two vectors measures the straight-line distance between the two points in this feature space.\n",
    "\n",
    "Word Mover's Distance (WMD) is a distance metric that measures the dissimilarity between two text documents. It is based on the concept of \"word embeddings\", which are representations of words in a high-dimensional vector space, where each dimension corresponds to a feature of the word.\n",
    "\n",
    "We later decided to apply Cosine, Jaccard, Levenshtein similarities and Euclidean distance, which were proven to work better for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14ed4a68",
   "metadata": {},
   "source": [
    "I also worked on understanding which features to use in the preprocessing to find similiarities between the questions. I did this by running the models and adding some of the features in the preprocessing while comparing the results without adding them.\n",
    "\n",
    "I also tried to understand how to add a syntactic analysis as a feature to compare the questions, but increased the computing times.\n",
    "I tried POS (part-of-speach) from spaCy library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a199a74e",
   "metadata": {},
   "source": [
    "Then, I basically started to try different combinations of parameters for the preprocessing, feature extraction and modelling, to see which models worked better in the different proposals I did.\n",
    "\n",
    "Some of the examples we tried are shwn in the pdf attached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "770bf7c0",
   "metadata": {},
   "source": [
    "It is also important to remark that as it was the first time working with classes in Python (I am an statistician and I have always coded in R), and also my first time working in a Python environment collavoratively I had to spend a lot of my time trying to understand how every file worked and how the connections were made."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40d1bd33",
   "metadata": {},
   "source": [
    "That's why, I concentrated on the results file, were I explained all the steps we have followed in this project, until providing the best model we could, which took me most of the time I have spent in this project\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf0d5f15",
   "metadata": {},
   "source": [
    "# FURTHER STEPS OF IMPROVEMENT\n",
    "\n",
    "I think adding a syntactic function would help to find similiarities between questions. I would try to find a way to add a syntactic function without increasing the time consumption as significantly as when we tried. \n",
    "\n",
    "As we had some problems with overfitting (even though we did cross-validation) I also think it would be great to apply some procedures to avoid it as Data Augmentation, Regularization,... I think the problem is that we applied the cross validation with only 2 folds because of the computation time (we wanted to do a grid search for all functions and models). So even though we tried to avoid it, as the computation time was so high we ended up reducing the folds and then overfitting appeared."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
